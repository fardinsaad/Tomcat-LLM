{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea83d6a4-dcc2-4186-a7ac-7564c3c55b5d",
   "metadata": {},
   "source": [
    "# Environment setup\n",
    "## Reload and import agent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b8d7d-9fd4-47ad-8def-ba20dc20dfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import agent\n",
    "importlib.reload(agent)\n",
    "from agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7b55b3-3354-4e92-9b4a-1ba899dc70dc",
   "metadata": {},
   "source": [
    "## Reload and import env_setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "494fbd67-6d1c-4100-aabb-26d7fd721afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import env_setup\n",
    "importlib.reload(env_setup)\n",
    "from env_setup import GridEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36353e34",
   "metadata": {},
   "source": [
    "# Individual Instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "656505a3-ac38-4d1e-975f-cd13f6793f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . y . . . b W W W W\n",
      "r W W r W W . r W W W g\n",
      "W W W W W W m W W W W R\n",
      "W W W W W W . W W W W .\n",
      "g . . . B . . R . . . .\n",
      "W W W W W W . W W W W W\n",
      "W W W W W W . W W W W W\n",
      ". . . Y . . . W W W W W\n",
      "B W W W W W . W W W W W\n",
      "g W W W W W h . . . . g\n",
      "Generating chain-of-thought prompt...\n",
      "Response: ```\n",
      "Human Action: The human moves upward from their current position at (9,6) to (4,6), which is adjacent to the red door at (4,7). Upon arriving at (4,6), they provide the instruction.\n",
      "Instruction: Give me a red key for this door?\n",
      "Type: Unclear. Based on the instruction and the grid, I infer that the human wants to collect the gem at (4,0), which is beyond the red door at (4,7). However, there is another red door at (2,11) blocking the path to the gem at (1,11). Therefore, the human might need two red keys. The instruction only asks for one red key, lacking specificity about the second red door.\n",
      "Response: There are three red keys and two red doors on the grid. The instruction asks for a red key to unlock the door at (4,7). However, there is another red door at (2,11) blocking the path to the gem at (1,11). Thus, I will collect two red keys, one for the door at (4,7) and another for the door at (2,11). To minimize movement, I will first pick up the red key at (1,0) followed by the red key at (1,7), as this strategy reduces the number of steps from my current position at (2,6). I will then pass them to the human at (4,6) to unlock the doors and collect the gem.\n",
      "Actions:\n",
      "1) Collect: red_key at (1,0).\n",
      "2) Collect: red_key at (1,7).\n",
      "3) Pass: red_keys to the human at (4,6).\n",
      "4) Unlock: human unlocks the Red_door at (4,7) and the Red_door at (2,11).\n",
      "5) Retrieve: human retrieves gem at (1,11).\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment and agent\n",
    "env = GridEnvironment()\n",
    "name = \"3.txt\"  # Change this to the name of the grid file you want to load like \"1.txt\", \"2.txt\", etc.\n",
    "grid_file_path = env.get_grid_file_path(name)\n",
    "\n",
    "if grid_file_path:\n",
    "    env.load_grid_from_file(grid_file_path)\n",
    "    env.display_grid()\n",
    "\n",
    "# Get agent and object positions\n",
    "agent_pos = env.get_agent_position()\n",
    "exp = True # True for PaP, False for Fs-CoT\n",
    "llm = \"gemma3\"\n",
    "object_pos = env.get_all_object_positions()\n",
    "\n",
    "# Initialize the agent\n",
    "my_agent = Agent(agent_pos)\n",
    "\n",
    "# Provide instructions and test the LLM\n",
    "human_move = \"The human moves upward from their current position at (9,6) to (4,6), which is adjacent to the red door at (4,7). Upon arriving at (4,6), they provide the instruction.\" # Change the human move to test different cases\n",
    "instruction = \"Give me a red key for this door?\" # Change the instruction to test different cases\n",
    "response = my_agent.chain_of_thought_prompt(instruction, env.grid, human_move, object_pos, exp, llm)\n",
    "print(\"Response:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7d306f",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eb437e",
   "metadata": {},
   "source": [
    "## Experiment 1: Tomcat with PaP (GPT-4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2408801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file from the local directory\n",
    "csv_file_path = 'ToM_PaP-dataset.csv'  \n",
    "df = pd.read_csv(csv_file_path)\n",
    "#print(df.head())\n",
    "\n",
    "# Ensure the 'Generated_response_gpt4o' column exists and is of type string\n",
    "if 'Generated_response_gpt4o' not in df.columns:\n",
    "    df['Generated_response_gpt4o'] = \"\"  # Initialize with empty strings\n",
    "else:\n",
    "    df['Generated_response_gpt4o'] = df['Generated_response_gpt4o'].astype(str)\n",
    "\n",
    "# num_rows = 1  # Adjust this to process 5 or 10 rows, or any other number\n",
    "# df_subset = df.head(num_rows)\n",
    "\n",
    "# Loop through each row, read the first and second columns, and generate the response\n",
    "for index, row in df.iterrows():\n",
    "    human = row.iloc[0]\n",
    "    instruction = row.iloc[1]  \n",
    "    file_name = row.iloc[2]   \n",
    "\n",
    "\n",
    "    # Initialize the environment and agent for the single file\n",
    "    env = GridEnvironment()\n",
    "    grid_file_path = env.get_grid_file_path(file_name)\n",
    "\n",
    "    if grid_file_path:\n",
    "        env.load_grid_from_file(grid_file_path)\n",
    "        # env.display_grid()\n",
    "\n",
    "        # Get agent and object positions\n",
    "        agent_pos = env.get_agent_position()\n",
    "        object_pos = env.get_all_object_positions()\n",
    "\n",
    "        # Initialize the agent\n",
    "        my_agent = Agent(agent_pos)\n",
    "        \n",
    "        # Experiment flag\n",
    "        exp = True\n",
    "        #LLM used\n",
    "        llm = \"gpt-4o\"\n",
    "\n",
    "        # Generate the response using the agent and instruction\n",
    "        response = my_agent.chain_of_thought_prompt(instruction, env.grid, human, object_pos, exp, llm)\n",
    "        \n",
    "        # Append the generated response directly into the DataFrame\n",
    "        df.at[index, 'Generated_response_gpt4o'] = str(response)\n",
    "\n",
    "# # After processing the subset, save the updated DataFrame to the CSV file\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e896d0",
   "metadata": {},
   "source": [
    "## Experiment 2: Tomcat with Fs-CoT (GPT-4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be0870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file from the local directory\n",
    "csv_file_path = 'ToM_FsCoT-dataset.csv'  \n",
    "df = pd.read_csv(csv_file_path)\n",
    "#print(df.head())\n",
    "\n",
    "# Ensure the 'Generated_response_gpt4o' column exists and is of type string\n",
    "if 'Generated_response_gpt4o' not in df.columns:\n",
    "    df['Generated_response_gpt4o'] = \"\"  # Initialize with empty strings\n",
    "else:\n",
    "    df['Generated_response_gpt4o'] = df['Generated_response_gpt4o'].astype(str)\n",
    "\n",
    "# num_rows = 1  # Adjust this to process 5 or 10 rows, or any other number\n",
    "# df_subset = df.head(num_rows)\n",
    "\n",
    "# Loop through each row, read the first and second columns, and generate the response\n",
    "for index, row in df.iterrows():\n",
    "    human = row.iloc[0]\n",
    "    instruction = row.iloc[1]  \n",
    "    file_name = row.iloc[2]   \n",
    "\n",
    "\n",
    "    # Initialize the environment and agent for the single file\n",
    "    env = GridEnvironment()\n",
    "    grid_file_path = env.get_grid_file_path(file_name)\n",
    "\n",
    "    if grid_file_path:\n",
    "        env.load_grid_from_file(grid_file_path)\n",
    "        # env.display_grid()\n",
    "\n",
    "        # Get agent and object positions\n",
    "        agent_pos = env.get_agent_position()\n",
    "        object_pos = env.get_all_object_positions()\n",
    "\n",
    "        # Initialize the agent\n",
    "        my_agent = Agent(agent_pos)\n",
    "        \n",
    "        # Experiment flag\n",
    "        exp = False\n",
    "        #LLM used\n",
    "        llm = \"gpt-4o\"\n",
    "\n",
    "        # Generate the response using the agent and instruction\n",
    "        response = my_agent.chain_of_thought_prompt(instruction, env.grid, human, object_pos, exp, llm)\n",
    "        \n",
    "        # Append the generated response directly into the DataFrame\n",
    "        df.at[index, 'Generated_response_gpt4o'] = str(response)\n",
    "\n",
    "# # After processing the subset, save the updated DataFrame to the CSV file\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18a4c91",
   "metadata": {},
   "source": [
    "## Experiment 3: Tomcat with PaP (Gemma3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f264b836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Sleeping for 60 seconds after 2 rows...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Sleeping for 60 seconds after 4 rows...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Sleeping for 60 seconds after 6 rows...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Sleeping for 60 seconds after 8 rows...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Sleeping for 60 seconds after 10 rows...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Sleeping for 60 seconds after 12 rows...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Sleeping for 60 seconds after 14 rows...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Sleeping for 60 seconds after 16 rows...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Sleeping for 60 seconds after 18 rows...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Sleeping for 60 seconds after 20 rows...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Load the CSV file from the local directory\n",
    "csv_file_path = 'ToM_PaP-dataset.csv'  \n",
    "df = pd.read_csv(csv_file_path)\n",
    "#print(df.head())\n",
    "\n",
    "# Ensure the 'Generated_response_gemma3' column exists and is of type string\n",
    "if 'Generated_response_gemma3' not in df.columns:\n",
    "    df['Generated_response_gemma3'] = \"\"  # Initialize with empty strings\n",
    "else:\n",
    "    df['Generated_response_gemma3'] = df['Generated_response_gemma3'].astype(str)\n",
    "\n",
    "# num_rows = 3  # Adjust this to process 5 or 10 rows, or any other number\n",
    "# df_subset = df.head(num_rows)\n",
    "\n",
    "batch_size = 2  # Number of rows to process in each batch\n",
    "sleep_time = 60  # Sleep time in seconds between batches\n",
    "counter = 0 \n",
    "\n",
    "# Loop through each row, read the first and second columns, and generate the response\n",
    "for index, row in df.iterrows():\n",
    "    human = row.iloc[0]\n",
    "    instruction = row.iloc[1]  \n",
    "    file_name = row.iloc[2]   \n",
    "\n",
    "\n",
    "    # Initialize the environment and agent for the single file\n",
    "    env = GridEnvironment()\n",
    "    grid_file_path = env.get_grid_file_path(file_name)\n",
    "\n",
    "    if grid_file_path:\n",
    "        env.load_grid_from_file(grid_file_path)\n",
    "        # env.display_grid()\n",
    "\n",
    "        # Get agent and object positions\n",
    "        agent_pos = env.get_agent_position()\n",
    "        object_pos = env.get_all_object_positions()\n",
    "\n",
    "        # Initialize the agent\n",
    "        my_agent = Agent(agent_pos)\n",
    "        \n",
    "        # Experiment flag\n",
    "        exp = True\n",
    "        #LLM used\n",
    "        llm = \"gemma3\"\n",
    "\n",
    "        # Generate the response using the agent and instruction\n",
    "        response = my_agent.chain_of_thought_prompt(instruction, env.grid, human, object_pos, exp, llm)\n",
    "        \n",
    "        # Append the generated response directly into the DataFrame\n",
    "        df.at[index, 'Generated_response_gemma3'] = response\n",
    "        counter += 1\n",
    "        if counter % batch_size == 0:\n",
    "            print(f\"Sleeping for {sleep_time} seconds after {counter} rows...\")\n",
    "            time.sleep(sleep_time)\n",
    "        \n",
    "\n",
    "# # After processing the subset, save the updated DataFrame to the CSV file\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba3399c",
   "metadata": {},
   "source": [
    "## Experiment 4: Tomcat with Fs-CoT (Gemma3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f2d9773b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Sleeping for 60 seconds after 2 rows...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Sleeping for 60 seconds after 4 rows...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Sleeping for 60 seconds after 6 rows...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Sleeping for 60 seconds after 8 rows...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Sleeping for 60 seconds after 10 rows...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Sleeping for 60 seconds after 12 rows...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Sleeping for 60 seconds after 14 rows...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Sleeping for 60 seconds after 16 rows...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Sleeping for 60 seconds after 18 rows...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Sleeping for 60 seconds after 20 rows...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Load the CSV file from the local directory\n",
    "csv_file_path = 'ToM_FsCoT-dataset.csv'  \n",
    "df = pd.read_csv(csv_file_path)\n",
    "#print(df.head())\n",
    "\n",
    "# Ensure the 'Generated_response_gemma3' column exists and is of type string\n",
    "if 'Generated_response_gemma3' not in df.columns:\n",
    "    df['Generated_response_gemma3'] = \"\"  # Initialize with empty strings\n",
    "else:\n",
    "    df['Generated_response_gemma3'] = df['Generated_response_gemma3'].astype(str)\n",
    "\n",
    "# num_rows = 1  # Adjust this to process 5 or 10 rows, or any other number\n",
    "# df_subset = df.head(num_rows)\n",
    "\n",
    "batch_size = 2  # Number of rows to process in each batch\n",
    "sleep_time = 60  # Sleep time in seconds between batches\n",
    "counter = 0 \n",
    "\n",
    "# Loop through each row, read the first and second columns, and generate the response\n",
    "for index, row in df.iterrows():\n",
    "    human = row.iloc[0]\n",
    "    instruction = row.iloc[1]  \n",
    "    file_name = row.iloc[2]   \n",
    "\n",
    "\n",
    "    # Initialize the environment and agent for the single file\n",
    "    env = GridEnvironment()\n",
    "    grid_file_path = env.get_grid_file_path(file_name)\n",
    "\n",
    "    if grid_file_path:\n",
    "        env.load_grid_from_file(grid_file_path)\n",
    "        # env.display_grid()\n",
    "\n",
    "        # Get agent and object positions\n",
    "        agent_pos = env.get_agent_position()\n",
    "        object_pos = env.get_all_object_positions()\n",
    "\n",
    "        # Initialize the agent\n",
    "        my_agent = Agent(agent_pos)\n",
    "        \n",
    "        # Experiment flag\n",
    "        exp = False\n",
    "        #LLM used\n",
    "        llm = \"gemma3\"\n",
    "\n",
    "        # Generate the response using the agent and instruction\n",
    "        response = my_agent.chain_of_thought_prompt(instruction, env.grid, human, object_pos, exp, llm)\n",
    "        \n",
    "        # Append the generated response directly into the DataFrame\n",
    "        df.at[index, 'Generated_response_gemma3'] = response\n",
    "        \n",
    "        counter += 1\n",
    "        if counter % batch_size == 0:\n",
    "            print(f\"Sleeping for {sleep_time} seconds after {counter} rows...\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "# # After processing the subset, save the updated DataFrame to the CSV file\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10976a61",
   "metadata": {},
   "source": [
    "## Experiment 5: Tomcat with PaP (Deepseek-R1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8cc9f3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n",
      "Generating chain-of-thought prompt...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# import time\n",
    "\n",
    "# Load the CSV file from the local directory\n",
    "csv_file_path = 'ToM_PaP-dataset.csv'  \n",
    "df = pd.read_csv(csv_file_path)\n",
    "#print(df.head())\n",
    "\n",
    "# Ensure the 'Generated_response_gemma3' column exists and is of type string\n",
    "if 'Generated_response_R1' not in df.columns:\n",
    "    df['Generated_response_R1'] = \"\"  # Initialize with empty strings\n",
    "else:\n",
    "    df['Generated_response_R1'] = df['Generated_response_R1'].astype(str)\n",
    "\n",
    "# num_rows = 3  # Adjust this to process 5 or 10 rows, or any other number\n",
    "# df_subset = df.head(num_rows)\n",
    "\n",
    "batch_size = 2  # Number of rows to process in each batch\n",
    "sleep_time = 60  # Sleep time in seconds between batches\n",
    "counter = 0 \n",
    "\n",
    "# Loop through each row, read the first and second columns, and generate the response\n",
    "for index, row in df.iterrows():\n",
    "    human = row.iloc[0]\n",
    "    instruction = row.iloc[1]  \n",
    "    file_name = row.iloc[2]   \n",
    "\n",
    "\n",
    "    # Initialize the environment and agent for the single file\n",
    "    env = GridEnvironment()\n",
    "    grid_file_path = env.get_grid_file_path(file_name)\n",
    "\n",
    "    if grid_file_path:\n",
    "        env.load_grid_from_file(grid_file_path)\n",
    "        # env.display_grid()\n",
    "\n",
    "        # Get agent and object positions\n",
    "        agent_pos = env.get_agent_position()\n",
    "        object_pos = env.get_all_object_positions()\n",
    "\n",
    "        # Initialize the agent\n",
    "        my_agent = Agent(agent_pos)\n",
    "        \n",
    "        # Experiment flag\n",
    "        exp = True\n",
    "        #LLM used\n",
    "        llm = \"R1\"\n",
    "\n",
    "        # Generate the response using the agent and instruction\n",
    "        response = my_agent.chain_of_thought_prompt(instruction, env.grid, human, object_pos, exp, llm)\n",
    "        \n",
    "        # Append the generated response directly into the DataFrame\n",
    "        df.at[index, 'Generated_response_R1'] = response\n",
    "        # counter += 1\n",
    "        # if counter % batch_size == 0:\n",
    "        #     print(f\"Sleeping for {sleep_time} seconds after {counter} rows...\")\n",
    "        #     time.sleep(sleep_time)\n",
    "        \n",
    "\n",
    "# # After processing the subset, save the updated DataFrame to the CSV file\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4c8f32",
   "metadata": {},
   "source": [
    "## Experiment 6: Tomcat with Fs-Cot (Deepseek-R1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52abb846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating chain-of-thought prompt...\n",
      "Counter: 1\n",
      "Generating chain-of-thought prompt...\n",
      "Counter: 2\n",
      "Generating chain-of-thought prompt...\n",
      "Counter: 3\n",
      "Generating chain-of-thought prompt...\n",
      "Counter: 4\n",
      "Generating chain-of-thought prompt...\n",
      "Counter: 5\n",
      "Generating chain-of-thought prompt...\n",
      "Counter: 6\n",
      "Generating chain-of-thought prompt...\n",
      "Counter: 7\n",
      "Generating chain-of-thought prompt...\n",
      "Counter: 8\n",
      "Generating chain-of-thought prompt...\n",
      "Counter: 9\n",
      "Generating chain-of-thought prompt...\n",
      "Counter: 10\n",
      "Generating chain-of-thought prompt...\n",
      "Counter: 11\n",
      "Generating chain-of-thought prompt...\n",
      "Counter: 12\n",
      "Generating chain-of-thought prompt...\n",
      "Counter: 13\n",
      "Generating chain-of-thought prompt...\n",
      "Counter: 14\n",
      "Generating chain-of-thought prompt...\n",
      "Counter: 15\n",
      "Generating chain-of-thought prompt...\n",
      "Counter: 16\n",
      "Generating chain-of-thought prompt...\n",
      "Counter: 17\n",
      "Generating chain-of-thought prompt...\n",
      "Counter: 18\n",
      "Generating chain-of-thought prompt...\n",
      "Counter: 19\n",
      "Generating chain-of-thought prompt...\n",
      "Counter: 20\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# import time\n",
    "\n",
    "# Load the CSV file from the local directory\n",
    "csv_file_path = 'ToM_FsCoT-dataset.csv'  \n",
    "df = pd.read_csv(csv_file_path)\n",
    "#print(df.head())\n",
    "\n",
    "# Ensure the 'Generated_response_gemma3' column exists and is of type string\n",
    "if 'Generated_response_R1' not in df.columns:\n",
    "    df['Generated_response_R1'] = \"\"  # Initialize with empty strings\n",
    "else:\n",
    "    df['Generated_response_R1'] = df['Generated_response_R1'].astype(str)\n",
    "\n",
    "# num_rows = 3  # Adjust this to process 5 or 10 rows, or any other number\n",
    "# df_subset = df.head(num_rows)\n",
    "\n",
    "batch_size = 2  # Number of rows to process in each batch\n",
    "sleep_time = 60  # Sleep time in seconds between batches\n",
    "counter = 0 \n",
    "\n",
    "# Loop through each row, read the first and second columns, and generate the response\n",
    "for index, row in df.iterrows():\n",
    "    human = row.iloc[0]\n",
    "    instruction = row.iloc[1]  \n",
    "    file_name = row.iloc[2]   \n",
    "\n",
    "\n",
    "    # Initialize the environment and agent for the single file\n",
    "    env = GridEnvironment()\n",
    "    grid_file_path = env.get_grid_file_path(file_name)\n",
    "\n",
    "    if grid_file_path:\n",
    "        env.load_grid_from_file(grid_file_path)\n",
    "        # env.display_grid()\n",
    "\n",
    "        # Get agent and object positions\n",
    "        agent_pos = env.get_agent_position()\n",
    "        object_pos = env.get_all_object_positions()\n",
    "\n",
    "        # Initialize the agent\n",
    "        my_agent = Agent(agent_pos)\n",
    "        \n",
    "        # Experiment flag\n",
    "        exp = False\n",
    "        #LLM used\n",
    "        llm = \"R1\"\n",
    "\n",
    "        # Generate the response using the agent and instruction\n",
    "        response = my_agent.chain_of_thought_prompt(instruction, env.grid, human, object_pos, exp, llm)\n",
    "        \n",
    "        # Append the generated response directly into the DataFrame\n",
    "        df.at[index, 'Generated_response_R1'] = response\n",
    "        counter += 1\n",
    "        print(f\"Counter: {counter}\")\n",
    "        # if counter % batch_size == 0:\n",
    "        #     print(f\"Sleeping for {sleep_time} seconds after {counter} rows...\")\n",
    "        #     time.sleep(sleep_time)\n",
    "        \n",
    "\n",
    "# # After processing the subset, save the updated DataFrame to the CSV file\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
